,Title,URL,Date,Abstruct,Category
0,"Efficient Fine-Tuning with Domain Adaptation for Privacy-Preserving
  Vision Transformer",http://arxiv.org/abs/2401.05126v2,2024-01-10T12:46:31Z,"We propose a novel method for privacy-preserving deep neural networks (DNNs)
with the Vision Transformer (ViT). The method allows us not only to train
models and test with visually protected images but to also avoid the
performance degradation caused from the use of encrypted images, whereas
conventional methods cannot avoid the influence of image encryption. A domain
adaptation method is used to efficiently fine-tune ViT with encrypted images.
In experiments, the method is demonstrated to outperform conventional methods
in an image classification task on the CIFAR-10 and ImageNet datasets in terms
of classification accuracy.",cs.CV
1,Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain,http://arxiv.org/abs/2402.06190v1,2024-02-09T05:06:58Z,"Standard modern machine-learning-based imaging methods have faced challenges
in medical applications due to the high cost of dataset construction and,
thereby, the limited labeled training data available. Additionally, upon
deployment, these methods are usually used to process a large volume of data on
a daily basis, imposing a high maintenance cost on medical facilities. In this
paper, we introduce a new neural network architecture, termed LoGoNet, with a
tailored self-supervised learning (SSL) method to mitigate such challenges.
LoGoNet integrates a novel feature extractor within a U-shaped architecture,
leveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture
both long-range and short-range feature dependencies adeptly. This is in
contrast to existing methods that rely on increasing network capacity to
enhance feature extraction. This combination of novel techniques in our model
is especially beneficial in medical image segmentation, given the difficulty of
learning intricate and often irregular body organ shapes, such as the spleen.
Complementary, we propose a novel SSL method tailored for 3D images to
compensate for the lack of large labeled datasets. The method combines masking
and contrastive learning techniques within a multi-task learning framework and
is compatible with both Vision Transformer (ViT) and CNN-based models. We
demonstrate the efficacy of our methods in numerous tasks across two standard
datasets (i.e., BTCV and MSD). Benchmark comparisons with eight
state-of-the-art models highlight LoGoNet's superior performance in both
inference time and accuracy.",cs.CV
2,"NeRCC: Nested-Regression Coded Computing for Resilient Distributed
  Prediction Serving Systems",http://arxiv.org/abs/2402.04377v2,2024-02-06T20:31:15Z,"Resilience against stragglers is a critical element of prediction serving
systems, tasked with executing inferences on input data for a pre-trained
machine-learning model. In this paper, we propose NeRCC, as a general
straggler-resistant framework for approximate coded computing. NeRCC includes
three layers: (1) encoding regression and sampling, which generates coded data
points, as a combination of original data points, (2) computing, in which a
cluster of workers run inference on the coded data points, (3) decoding
regression and sampling, which approximately recovers the predictions of the
original data points from the available predictions on the coded data points.
We argue that the overall objective of the framework reveals an underlying
interconnection between two regression models in the encoding and decoding
layers. We propose a solution to the nested regressions problem by summarizing
their dependence on two regularization terms that are jointly optimized. Our
extensive experiments on different datasets and various machine learning
models, including LeNet5, RepVGG, and Vision Transformer (ViT), demonstrate
that NeRCC accurately approximates the original predictions in a wide range of
stragglers, outperforming the state-of-the-art by up to 23%.",cs.LG
3,"When CNN Meet with ViT: Towards Semi-Supervised Learning for Multi-Class
  Medical Image Semantic Segmentation",http://arxiv.org/abs/2208.06449v2,2022-08-12T18:21:22Z,"Due to the lack of quality annotation in medical imaging community,
semi-supervised learning methods are highly valued in image semantic
segmentation tasks. In this paper, an advanced consistency-aware
pseudo-label-based self-ensembling approach is presented to fully utilize the
power of Vision Transformer(ViT) and Convolutional Neural Network(CNN) in
semi-supervised learning. Our proposed framework consists of a feature-learning
module which is enhanced by ViT and CNN mutually, and a guidance module which
is robust for consistency-aware purposes. The pseudo labels are inferred and
utilized recurrently and separately by views of CNN and ViT in the
feature-learning module to expand the data set and are beneficial to each
other. Meanwhile, a perturbation scheme is designed for the feature-learning
module, and averaging network weight is utilized to develop the guidance
module. By doing so, the framework combines the feature-learning strength of
CNN and ViT, strengthens the performance via dual-view co-training, and enables
consistency-aware supervision in a semi-supervised manner. A topological
exploration of all alternative supervision modes with CNN and ViT are detailed
validated, demonstrating the most promising performance and specific setting of
our method on semi-supervised medical image segmentation tasks. Experimental
results show that the proposed method achieves state-of-the-art performance on
a public benchmark data set with a variety of metrics. The code is publicly
available.",eess.IV
4,"Memory-Efficient Vision Transformers: An Activation-Aware Mixed-Rank
  Compression Strategy",http://arxiv.org/abs/2402.06004v1,2024-02-08T19:01:14Z,"As Vision Transformers (ViTs) increasingly set new benchmarks in computer
vision, their practical deployment on inference engines is often hindered by
their significant memory bandwidth and (on-chip) memory footprint requirements.
This paper addresses this memory limitation by introducing an activation-aware
model compression methodology that uses selective low-rank weight tensor
approximations of different layers to reduce the parameter count of ViTs. The
key idea is to decompose the weight tensors into a sum of two
parameter-efficient tensors while minimizing the error between the product of
the input activations with the original weight tensor and the product of the
input activations with the approximate tensor sum. This approximation is
further refined by adopting an efficient layer-wise error compensation
technique that uses the gradient of the layer's output loss. The combination of
these techniques achieves excellent results while it avoids being trapped in a
shallow local minimum early in the optimization process and strikes a good
balance between the model compression and output accuracy. Notably, the
presented method significantly reduces the parameter count of DeiT-B by 60%
with less than 1% accuracy drop on the ImageNet dataset, overcoming the usual
accuracy degradation seen in low-rank approximations. In addition to this, the
presented compression technique can compress large DeiT/ViT models to have
about the same model size as smaller DeiT/ViT variants while yielding up to
1.8% accuracy gain. These results highlight the efficacy of our approach,
presenting a viable solution for embedding ViTs in memory-constrained
environments without compromising their performance.",cs.CV
